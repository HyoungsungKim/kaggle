{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit66f7a8720ea44564891eb6b9b39e6c03",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import time\n",
    "\n",
    "from random import choice\n",
    "from kaggle_environments import evaluate, make, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['observation', 'action', 'reward', 'done', 'new_observation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # * Get a number of batch_size experience in a range of len(self.buffer)\n",
    "        # * It does not replace\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "\n",
    "        observation, actions, rewards, dones, next_observation = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(observation), np.array(actions), np.array(rewards, dtype=np.float32), np.array(dones, dtype=np.uint8), np.array(next_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, shape):\n",
    "        return shape.view(shape.size()[0], -1)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=3, padding = 1, stride=4),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=4, padding = 1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding = 1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, input_shape):\n",
    "        # * To make 3D shape, we have to put 1 in torch.zeros\n",
    "        # * torch.zeros(1, 1, 52, 52)\n",
    "        # * -> troch.zeros(batch_dimension, *shape)\n",
    "        out = self.conv(torch.zeros(1, *input_shape))\n",
    "        return int(np.prod(out.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x)\n",
    "        fc_out = self.fc(conv_out)\n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "DQN(\n  (conv): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): ReLU()\n    (6): Flatten()\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=2304, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n"
    },
    {
     "data": {
      "text/plain": "tensor([[-0.0269,  0.0056, -0.0218, -0.0331, -0.0191,  0.0173, -0.0209,  0.0149,\n         -0.0214, -0.0019]], grad_fn=<AddmmBackward>)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(1, 1, 52, 52)\n",
    "dqn = DQN(image[0].shape, 10)\n",
    "print(dqn)\n",
    "dqn(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /kaggle_environments/envs/connectx.py\n",
    "def is_win(board, column, mark, config, has_played=True):\n",
    "    columns = config.columns\n",
    "    rows = config.rows\n",
    "    inarow = config.inarow - 1\n",
    "    row = (\n",
    "        min([r for r in range(rows) if board[column + (r * columns)] == mark])\n",
    "        if has_played\n",
    "        else max([r for r in range(rows) if board[column + (r * columns)] == 0])\n",
    "    )\n",
    "\n",
    "    def count(offset_row, offset_column):\n",
    "        for i in range(1, inarow + 1):\n",
    "            r = row + offset_row * i\n",
    "            c = column + offset_column * i\n",
    "            if (\n",
    "                r < 0\n",
    "                or r >= rows\n",
    "                or c < 0\n",
    "                or c >= columns\n",
    "                or board[c + (r * columns)] != mark\n",
    "            ):\n",
    "                return i - 1\n",
    "        return inarow\n",
    "\n",
    "    return (\n",
    "        count(1, 0) >= inarow  # vertical.\n",
    "        or (count(0, 1) + count(0, -1)) >= inarow  # horizontal.\n",
    "        or (count(-1, -1) + count(1, 1)) >= inarow  # top left diagonal.\n",
    "        or (count(-1, 1) + count(1, -1)) >= inarow  # top right diagonal.\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer):\n",
    "        configuration = env.configuration\n",
    "        self.env = env\n",
    "        self.columns = configuration['columns']\n",
    "        self.rows = configuration['rows']        \n",
    "        self.exp_buffer = exp_buffer\n",
    "        self.epsilon = 1.0\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            self.trainer = self.env.train([None, \"random\"])\n",
    "           # print(\"random mode\")\n",
    "            self.trainer.reset()\n",
    "        else:\n",
    "            self.trainer = self.env.train([None, \"negamax\"])\n",
    "           # print(\"negamax mode\")\n",
    "            self.trainer.reset()\n",
    "\n",
    "        env_observation = self.trainer.reset()\n",
    "        self.mark = env_observation['mark']\n",
    "        self.board = env_observation['board']\n",
    "        np_board = np.array(self.board)\n",
    "        np_board = np_board.reshape(1, self.rows, -1)\n",
    "        assert np_board.shape[2] == self.columns\n",
    "\n",
    "        self.env.reset()\n",
    "        self.observation = np_board\n",
    "        self.total_reward = 0.0        \n",
    "\n",
    "    def _select_random_action(self):   \n",
    "        return choice([c for c in range (self.columns) if self.board[c] == 0])  \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self._select_random_action()\n",
    "        else:            \n",
    "            observation_v = torch.from_numpy(self.observation).float().to(device)\n",
    "            # Unsqueeze for batch size\n",
    "            # [batch_size, channel, row, column]\n",
    "            q_vals_v = net(observation_v.unsqueeze(0))\n",
    "            _, action = torch.max(q_vals_v, dim=1)\n",
    "            action = int(action)\n",
    "\n",
    "        new_observation, reward, done, _ = self.trainer.step(action)        \n",
    "        \n",
    "        if reward == None:\n",
    "            done = True\n",
    "            reward = -10   \n",
    "\n",
    "        # Do not need to consider lose case. If agent can get high reward when it doen win        \n",
    "        if done is not True and is_win(self.board, action, self.mark, self.env.configuration, has_played=False):\n",
    "            reward = 10 \n",
    "\n",
    "        if done == False:\n",
    "            reward = -1                              \n",
    "   \n",
    "        self.total_reward += reward\n",
    "        self.board = new_observation['board']\n",
    "\n",
    "        new_observation = np.array(self.board)\n",
    "        new_observation = new_observation.reshape(1, self.rows, -1)\n",
    "        assert new_observation.shape[2] == self.columns\n",
    "        \n",
    "        exp = Experience(self.observation, action, reward, done, new_observation)\n",
    "\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.observation = new_observation\n",
    "\n",
    "        if done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()     \n",
    "\n",
    "        return done_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n\n"
    }
   ],
   "source": [
    "test_env = make(\"connectx\", debug=True)\n",
    "test_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = test_env.configuration['columns']\n",
    "rows = test_env.configuration['rows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "negamax mode\n"
    }
   ],
   "source": [
    "test_buffer = ExperienceBuffer(5)\n",
    "agent = Agent(test_env, test_buffer)\n",
    "epsilon = 0.5\n",
    "input_shape = [1, rows, columns]\n",
    "n_actions = columns\n",
    "net = DQN(input_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "None\nNone\nNone\nNone\nNone\nNone\nNone\nrandom mode\n-7.0\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nrandom mode\n-7.0\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nInvalid Action: Invalid column: 3\nnegamax mode\n-20.0\nNone\nNone\nNone\nnegamax mode\n-3.0\nNone\nNone\nNone\nnegamax mode\n-3.0\nNone\nNone\nNone\nNone\nnegamax mode\n-4.0\nNone\nNone\nNone\nnegamax mode\n-3.0\nNone\nNone\nNone\nNone\nrandom mode\n-4.0\nNone\nNone\nNone\nrandom mode\n-2.0\nNone\nNone\nNone\nNone\nNone\nnegamax mode\n-5.0\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nnegamax mode\n-9.0\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nInvalid Action: Invalid column: 3\nrandom mode\n-17.0\nNone\nNone\nNone\nNone\nNone\nNone\nnegamax mode\n-5.0\nNone\nNone\nNone\nNone\nnegamax mode\n-4.0\nNone\nNone\nNone\nNone\nNone\nrandom mode\n-5.0\nNone\nNone\nNone\nNone\n"
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    reward = agent.play_step(net, epsilon)\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 1 | 2 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 1 | 2 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 2 | 0 | 0 | 1 | 1 | 0 | 2 |\n+---+---+---+---+---+---+---+\n\n"
    }
   ],
   "source": [
    "test_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, tgt_net, device='cpu'):\n",
    "    observation, actions, rewards, dones, next_observation = batch\n",
    "\n",
    "    observation_v = torch.from_numpy(observation).float().to(device)\n",
    "    next_observation_v = torch.from_numpy(next_observation).float().to(device)\n",
    "    action_v = torch.from_numpy(actions).to(device)\n",
    "    rewards_v = torch.from_numpy(rewards).to(device)\n",
    "    done_mask = torch.from_numpy(dones).to(device)\n",
    "\n",
    "    state_action_value = net(observation_v).gather(1, action_v.unsqueeze(-1)).squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_observation_values = tgt_net(next_observation_v).max(1)[0]\n",
    "        next_observation_values[done_mask] = 0.0\n",
    "        next_observation_values = next_observation_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_observation_values * GAMMA + rewards_v\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    return loss(state_action_value, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "REPLAY_SIZE = 80000\n",
    "LEARNING_RATE = 1e-4\n",
    "SYNC_TARGET_FRAMES = 5000\n",
    "REPLAY_START_SIZE = 3000\n",
    "\n",
    "EPSILON_DECAY_LAST_FRAME = 5000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_FINAL = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Run using cuda\n"
    }
   ],
   "source": [
    "env = make(\"connectx\", debug=True)\n",
    "rows, columns = env.configuration['rows'], env.configuration['columns']\n",
    "input_shape = [1, rows, columns]\n",
    "n_actions = columns\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else 'cpu')\n",
    "print(f\"Run using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DQN(input_shape, n_actions).to(device)    \n",
    "tgt_net = DQN(input_shape, n_actions).to(device)\n",
    "\n",
    "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
    "agent = Agent(env, buffer)\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "total_rewards = []\n",
    "frame_idx = 0\n",
    "ts_frame = 0\n",
    "ts = time.time()\n",
    "best_m_reward = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "8 : done 1 games, reward -7.000, eps 1.00, speed 18.10 f/s\n23 : done 2 games, reward -10.500, eps 1.00, speed 330.58 f/s\n36 : done 3 games, reward -11.000, eps 0.99, speed 368.84 f/s\n45 : done 4 games, reward -10.250, eps 0.99, speed 350.53 f/s\n59 : done 5 games, reward -10.600, eps 0.99, speed 375.97 f/s\n68 : done 6 games, reward -10.000, eps 0.99, speed 266.50 f/s\n75 : done 7 games, reward -9.286, eps 0.98, speed 287.37 f/s\n83 : done 8 games, reward -9.000, eps 0.98, speed 328.25 f/s\n87 : done 9 games, reward -8.333, eps 0.98, speed 9.59 f/s\n100 : done 10 games, reward -8.700, eps 0.98, speed 343.20 f/s\n114 : done 11 games, reward -9.000, eps 0.98, speed 419.75 f/s\n127 : done 12 games, reward -9.167, eps 0.97, speed 429.09 f/s\n136 : done 13 games, reward -9.077, eps 0.97, speed 339.25 f/s\n143 : done 14 games, reward -8.857, eps 0.97, speed 311.52 f/s\n160 : done 15 games, reward -9.333, eps 0.97, speed 353.31 f/s\n169 : done 16 games, reward -9.188, eps 0.97, speed 267.50 f/s\n174 : done 17 games, reward -8.882, eps 0.97, speed 194.02 f/s\n179 : done 18 games, reward -8.556, eps 0.96, speed 243.77 f/s\n183 : done 19 games, reward -8.211, eps 0.96, speed 149.07 f/s\n193 : done 20 games, reward -8.250, eps 0.96, speed 359.66 f/s\n200 : done 21 games, reward -8.143, eps 0.96, speed 303.39 f/s\n209 : done 22 games, reward -8.136, eps 0.96, speed 361.25 f/s\n216 : done 23 games, reward -8.000, eps 0.96, speed 326.16 f/s\n224 : done 24 games, reward -7.917, eps 0.96, speed 344.10 f/s\n229 : done 25 games, reward -7.760, eps 0.95, speed 217.98 f/s\n240 : done 26 games, reward -7.846, eps 0.95, speed 274.35 f/s\n254 : done 27 games, reward -8.000, eps 0.95, speed 378.26 f/s\n265 : done 28 games, reward -8.036, eps 0.95, speed 376.05 f/s\n282 : done 29 games, reward -8.310, eps 0.94, speed 442.27 f/s\n297 : done 30 games, reward -8.467, eps 0.94, speed 338.31 f/s\n313 : done 31 games, reward -8.677, eps 0.94, speed 351.39 f/s\n320 : done 32 games, reward -8.594, eps 0.94, speed 283.75 f/s\n331 : done 33 games, reward -8.636, eps 0.93, speed 384.06 f/s\n339 : done 34 games, reward -8.559, eps 0.93, speed 227.10 f/s\n348 : done 35 games, reward -8.514, eps 0.93, speed 346.81 f/s\n359 : done 36 games, reward -8.556, eps 0.93, speed 376.35 f/s\n367 : done 37 games, reward -8.514, eps 0.93, speed 334.85 f/s\n378 : done 38 games, reward -8.553, eps 0.92, speed 375.58 f/s\n385 : done 39 games, reward -8.462, eps 0.92, speed 208.36 f/s\n399 : done 40 games, reward -8.550, eps 0.92, speed 333.64 f/s\n407 : done 41 games, reward -8.488, eps 0.92, speed 338.97 f/s\n416 : done 42 games, reward -8.476, eps 0.92, speed 225.98 f/s\n431 : done 43 games, reward -8.581, eps 0.91, speed 430.90 f/s\n437 : done 44 games, reward -8.477, eps 0.91, speed 293.72 f/s\n441 : done 45 games, reward -8.356, eps 0.91, speed 221.13 f/s\n451 : done 46 games, reward -8.370, eps 0.91, speed 371.48 f/s\n460 : done 47 games, reward -8.340, eps 0.91, speed 343.83 f/s\n473 : done 48 games, reward -8.417, eps 0.91, speed 16.70 f/s\n480 : done 49 games, reward -8.347, eps 0.90, speed 329.89 f/s\n485 : done 50 games, reward -8.240, eps 0.90, speed 289.24 f/s\n496 : done 51 games, reward -8.275, eps 0.90, speed 417.52 f/s\n512 : done 52 games, reward -8.385, eps 0.90, speed 498.60 f/s\n521 : done 53 games, reward -8.377, eps 0.90, speed 13.49 f/s\n526 : done 54 games, reward -8.278, eps 0.89, speed 272.15 f/s\n534 : done 55 games, reward -8.236, eps 0.89, speed 297.76 f/s\n539 : done 56 games, reward -8.143, eps 0.89, speed 295.92 f/s\n547 : done 57 games, reward -8.105, eps 0.89, speed 248.51 f/s\n552 : done 58 games, reward -8.034, eps 0.89, speed 11.23 f/s\n564 : done 59 games, reward -8.068, eps 0.89, speed 344.16 f/s\nInvalid Action: Invalid column: 6\n580 : done 60 games, reward -8.350, eps 0.88, speed 367.11 f/s\n589 : done 61 games, reward -8.328, eps 0.88, speed 365.10 f/s\n602 : done 62 games, reward -8.371, eps 0.88, speed 449.90 f/s\n611 : done 63 games, reward -8.365, eps 0.88, speed 359.37 f/s\n623 : done 64 games, reward -8.391, eps 0.88, speed 431.64 f/s\n636 : done 65 games, reward -8.431, eps 0.87, speed 359.85 f/s\n646 : done 66 games, reward -8.424, eps 0.87, speed 351.94 f/s\n667 : done 67 games, reward -8.597, eps 0.87, speed 20.74 f/s\n672 : done 68 games, reward -8.529, eps 0.87, speed 11.63 f/s\n681 : done 69 games, reward -8.522, eps 0.86, speed 371.56 f/s\nInvalid Action: Invalid column: 6\n701 : done 70 games, reward -8.814, eps 0.86, speed 496.73 f/s\n711 : done 71 games, reward -8.803, eps 0.86, speed 411.53 f/s\n726 : done 72 games, reward -8.861, eps 0.85, speed 453.25 f/s\n738 : done 73 games, reward -8.890, eps 0.85, speed 424.72 f/s\n748 : done 74 games, reward -8.892, eps 0.85, speed 400.63 f/s\n763 : done 75 games, reward -8.947, eps 0.85, speed 339.75 f/s\n771 : done 76 games, reward -8.908, eps 0.85, speed 340.29 f/s\n777 : done 77 games, reward -8.857, eps 0.84, speed 286.04 f/s\n781 : done 78 games, reward -8.782, eps 0.84, speed 10.60 f/s\n787 : done 79 games, reward -8.722, eps 0.84, speed 202.88 f/s\n791 : done 80 games, reward -8.637, eps 0.84, speed 197.66 f/s\nInvalid Action: Invalid column: 6\n805 : done 81 games, reward -8.815, eps 0.84, speed 450.30 f/s\n809 : done 82 games, reward -8.744, eps 0.84, speed 11.29 f/s\n816 : done 83 games, reward -8.699, eps 0.84, speed 345.40 f/s\n825 : done 84 games, reward -8.690, eps 0.83, speed 360.35 f/s\n829 : done 85 games, reward -8.624, eps 0.83, speed 167.21 f/s\nInvalid Action: Invalid column: 6\n843 : done 86 games, reward -8.791, eps 0.83, speed 387.73 f/s\n857 : done 87 games, reward -8.839, eps 0.83, speed 474.46 f/s\n871 : done 88 games, reward -8.875, eps 0.83, speed 458.47 f/s\nInvalid Action: Invalid column: 6\n880 : done 89 games, reward -8.978, eps 0.82, speed 12.33 f/s\n897 : done 90 games, reward -9.056, eps 0.82, speed 399.29 f/s\nInvalid Action: Invalid column: 6\n907 : done 91 games, reward -9.165, eps 0.82, speed 300.29 f/s\n914 : done 92 games, reward -9.130, eps 0.82, speed 311.17 f/s\n918 : done 93 games, reward -9.065, eps 0.82, speed 10.89 f/s\n933 : done 94 games, reward -9.117, eps 0.81, speed 327.17 f/s\n938 : done 95 games, reward -9.053, eps 0.81, speed 299.08 f/s\n942 : done 96 games, reward -8.990, eps 0.81, speed 11.60 f/s\n948 : done 97 games, reward -8.938, eps 0.81, speed 283.31 f/s\n957 : done 98 games, reward -8.929, eps 0.81, speed 255.75 f/s\n970 : done 99 games, reward -8.960, eps 0.81, speed 373.63 f/s\n975 : done 100 games, reward -8.910, eps 0.80, speed 11.65 f/s\n980 : done 101 games, reward -8.870, eps 0.80, speed 221.34 f/s\n984 : done 102 games, reward -8.760, eps 0.80, speed 11.03 f/s\n994 : done 103 games, reward -8.730, eps 0.80, speed 389.10 f/s\n1001 : done 104 games, reward -8.710, eps 0.80, speed 334.34 f/s\n1005 : done 105 games, reward -8.610, eps 0.80, speed 224.43 f/s\n1014 : done 106 games, reward -8.610, eps 0.80, speed 380.24 f/s\n1019 : done 107 games, reward -8.600, eps 0.80, speed 149.74 f/s\n1023 : done 108 games, reward -8.560, eps 0.80, speed 10.03 f/s\nInvalid Action: Invalid column: 6\n1036 : done 109 games, reward -8.750, eps 0.79, speed 340.75 f/s\n1043 : done 110 games, reward -8.690, eps 0.79, speed 313.44 f/s\n1049 : done 111 games, reward -8.610, eps 0.79, speed 282.76 f/s\n1070 : done 112 games, reward -8.695, eps 0.79, speed 17.62 f/s\n1074 : done 113 games, reward -8.645, eps 0.79, speed 10.41 f/s\nInvalid Action: Invalid column: 6\n1090 : done 114 games, reward -8.835, eps 0.78, speed 365.65 f/s\n1098 : done 115 games, reward -8.745, eps 0.78, speed 358.24 f/s\n1111 : done 116 games, reward -8.785, eps 0.78, speed 440.42 f/s\n1127 : done 117 games, reward -8.895, eps 0.77, speed 404.97 f/s\n1136 : done 118 games, reward -8.935, eps 0.77, speed 391.60 f/s\n1145 : done 119 games, reward -8.985, eps 0.77, speed 387.16 f/s\n1153 : done 120 games, reward -8.955, eps 0.77, speed 330.46 f/s\nInvalid Action: Invalid column: 6\n1160 : done 121 games, reward -9.055, eps 0.77, speed 292.37 f/s\n1168 : done 122 games, reward -9.045, eps 0.77, speed 309.17 f/s\n1172 : done 123 games, reward -9.025, eps 0.77, speed 9.36 f/s\n1187 : done 124 games, reward -9.105, eps 0.76, speed 452.14 f/s\n1200 : done 125 games, reward -9.175, eps 0.76, speed 373.38 f/s\n1205 : done 126 games, reward -9.115, eps 0.76, speed 9.83 f/s\n1217 : done 127 games, reward -9.105, eps 0.76, speed 403.34 f/s\n1227 : done 128 games, reward -9.105, eps 0.75, speed 312.35 f/s\nInvalid Action: Invalid column: 6\n1238 : done 129 games, reward -9.145, eps 0.75, speed 288.37 f/s\n1247 : done 130 games, reward -9.095, eps 0.75, speed 11.44 f/s\nInvalid Action: Invalid column: 6\n1259 : done 131 games, reward -9.155, eps 0.75, speed 367.54 f/s\n1266 : done 132 games, reward -9.145, eps 0.75, speed 242.46 f/s\nInvalid Action: Invalid column: 6\n1282 : done 133 games, reward -9.295, eps 0.74, speed 441.19 f/s\n1292 : done 134 games, reward -9.325, eps 0.74, speed 367.07 f/s\n1297 : done 135 games, reward -9.295, eps 0.74, speed 9.82 f/s\n1304 : done 136 games, reward -9.245, eps 0.74, speed 269.99 f/s\n1315 : done 137 games, reward -9.275, eps 0.74, speed 378.30 f/s\n1326 : done 138 games, reward -9.275, eps 0.73, speed 324.12 f/s\n1332 : done 139 games, reward -9.275, eps 0.73, speed 9.33 f/s\nInvalid Action: Invalid column: 6\n1348 : done 140 games, reward -9.405, eps 0.73, speed 385.00 f/s\n1352 : done 141 games, reward -9.365, eps 0.73, speed 222.11 f/s\n1356 : done 142 games, reward -9.305, eps 0.73, speed 215.78 f/s\n1371 : done 143 games, reward -9.315, eps 0.73, speed 352.15 f/s\n1376 : done 144 games, reward -9.305, eps 0.72, speed 279.51 f/s\n1384 : done 145 games, reward -9.335, eps 0.72, speed 335.96 f/s\n1392 : done 146 games, reward -9.305, eps 0.72, speed 340.92 f/s\n1397 : done 147 games, reward -9.275, eps 0.72, speed 12.28 f/s\n1401 : done 148 games, reward -9.185, eps 0.72, speed 10.33 f/s\n1411 : done 149 games, reward -9.215, eps 0.72, speed 385.46 f/s\n1418 : done 150 games, reward -9.235, eps 0.72, speed 239.99 f/s\nInvalid Action: Invalid column: 6\n1425 : done 151 games, reward -9.295, eps 0.72, speed 325.14 f/s\n1436 : done 152 games, reward -9.245, eps 0.71, speed 403.24 f/s\n1446 : done 153 games, reward -9.245, eps 0.71, speed 402.51 f/s\nInvalid Action: Invalid column: 6\n1455 : done 154 games, reward -9.395, eps 0.71, speed 235.16 f/s\nInvalid Action: Invalid column: 6\n1468 : done 155 games, reward -9.555, eps 0.71, speed 349.36 f/s\n1484 : done 156 games, reward -9.675, eps 0.70, speed 310.33 f/s\n1499 : done 157 games, reward -9.755, eps 0.70, speed 448.50 f/s\n1508 : done 158 games, reward -9.785, eps 0.70, speed 355.96 f/s\nInvalid Action: Invalid column: 6\n1520 : done 159 games, reward -9.895, eps 0.70, speed 409.93 f/s\n1524 : done 160 games, reward -9.675, eps 0.70, speed 10.04 f/s\n1536 : done 161 games, reward -9.715, eps 0.69, speed 400.76 f/s\n1541 : done 162 games, reward -9.635, eps 0.69, speed 257.52 f/s\n1549 : done 163 games, reward -9.625, eps 0.69, speed 327.15 f/s\n1553 : done 164 games, reward -9.555, eps 0.69, speed 10.35 f/s\n1559 : done 165 games, reward -9.485, eps 0.69, speed 294.61 f/s\nInvalid Action: Invalid column: 6\n1572 : done 166 games, reward -9.625, eps 0.69, speed 411.43 f/s\n1585 : done 167 games, reward -9.535, eps 0.68, speed 451.43 f/s\n1589 : done 168 games, reward -9.515, eps 0.68, speed 230.50 f/s\n1602 : done 169 games, reward -9.555, eps 0.68, speed 339.27 f/s\n1607 : done 170 games, reward -9.305, eps 0.68, speed 10.22 f/s\n1611 : done 171 games, reward -9.255, eps 0.68, speed 9.74 f/s\n1622 : done 172 games, reward -9.225, eps 0.68, speed 16.10 f/s\n1628 : done 173 games, reward -9.165, eps 0.67, speed 12.08 f/s\nInvalid Action: Invalid column: 6\n1641 : done 174 games, reward -9.295, eps 0.67, speed 14.23 f/s\n1650 : done 175 games, reward -9.235, eps 0.67, speed 250.53 f/s\nInvalid Action: Invalid column: 6\n1661 : done 176 games, reward -9.375, eps 0.67, speed 353.26 f/s\n1668 : done 177 games, reward -9.375, eps 0.67, speed 305.13 f/s\nInvalid Action: Invalid column: 6\n1681 : done 178 games, reward -9.565, eps 0.66, speed 297.84 f/s\n1685 : done 179 games, reward -9.555, eps 0.66, speed 9.59 f/s\nInvalid Action: Invalid column: 6\n1699 : done 180 games, reward -9.765, eps 0.66, speed 15.02 f/s\nInvalid Action: Invalid column: 6\n1714 : done 181 games, reward -9.775, eps 0.66, speed 330.46 f/s\n1718 : done 182 games, reward -9.775, eps 0.66, speed 10.00 f/s\nInvalid Action: Invalid column: 6\n1726 : done 183 games, reward -9.895, eps 0.65, speed 243.65 f/s\n1730 : done 184 games, reward -9.845, eps 0.65, speed 10.13 f/s\n1736 : done 185 games, reward -9.865, eps 0.65, speed 290.07 f/s\n1741 : done 186 games, reward -9.675, eps 0.65, speed 11.78 f/s\n1751 : done 187 games, reward -9.635, eps 0.65, speed 337.42 f/s\n1755 : done 188 games, reward -9.545, eps 0.65, speed 10.13 f/s\n1765 : done 189 games, reward -9.455, eps 0.65, speed 359.90 f/s\nInvalid Action: Invalid column: 6\n1776 : done 190 games, reward -9.495, eps 0.64, speed 358.02 f/s\n1785 : done 191 games, reward -9.375, eps 0.64, speed 324.42 f/s\nInvalid Action: Invalid column: 6\n1794 : done 192 games, reward -9.495, eps 0.64, speed 335.06 f/s\n1798 : done 193 games, reward -9.495, eps 0.64, speed 9.14 f/s\n1804 : done 194 games, reward -9.395, eps 0.64, speed 264.16 f/s\nInvalid Action: Invalid column: 6\n1814 : done 195 games, reward -9.555, eps 0.64, speed 272.60 f/s\n1819 : done 196 games, reward -9.565, eps 0.64, speed 12.03 f/s\n1825 : done 197 games, reward -9.575, eps 0.64, speed 179.78 f/s\n1833 : done 198 games, reward -9.565, eps 0.63, speed 329.76 f/s\n1837 : done 199 games, reward -9.475, eps 0.63, speed 9.71 f/s\n1846 : done 200 games, reward -9.515, eps 0.63, speed 12.07 f/s\n1855 : done 201 games, reward -9.555, eps 0.63, speed 365.33 f/s\n1860 : done 202 games, reward -9.565, eps 0.63, speed 9.52 f/s\n1864 : done 203 games, reward -9.505, eps 0.63, speed 9.30 f/s\nInvalid Action: Invalid column: 6\n1875 : done 204 games, reward -9.645, eps 0.62, speed 329.66 f/s\n1880 : done 205 games, reward -9.665, eps 0.62, speed 11.88 f/s\n1884 : done 206 games, reward -9.625, eps 0.62, speed 10.51 f/s\n1888 : done 207 games, reward -9.615, eps 0.62, speed 9.89 f/s\n1893 : done 208 games, reward -9.625, eps 0.62, speed 9.75 f/s\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-ccae2318db07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mframe_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPSILON_FINAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPSILON_START\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mframe_idx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mEPSILON_DECAY_LAST_FRAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtotal_rewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_no_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_no_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-723a74327e48>\u001b[0m in \u001b[0;36mplay_step\u001b[0;34m(self, net, epsilon, device)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mnew_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/kaggle_environments/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(action)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnone_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             return [\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/kaggle_environments/core.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m()\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"INACTIVE\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/kaggle_environments/core.py\u001b[0m in \u001b[0;36m__get_actions\u001b[0;34m(self, agents, none_action)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnone_action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_callable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 actions[i] = self.__run_agent(\n\u001b[0m\u001b[1;32m    441\u001b[0m                     self.agents[agent], self.state[i])\n\u001b[1;32m    442\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/kaggle_environments/core.py\u001b[0m in \u001b[0;36m__run_agent\u001b[0;34m(self, agent, state)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__code__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mco_argcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/kaggle_environments/utils.py\u001b[0m in \u001b[0;36mtimeout\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;31m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    frame_idx += 1\n",
    "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)   \n",
    "    reward = agent.play_step(net, epsilon, device=device)\n",
    "    if reward is not None:\n",
    "        total_rewards.append(reward)\n",
    "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
    "        ts_frame = frame_idx\n",
    "        ts = time.time()\n",
    "        m_reward = np.mean(total_rewards[-100:])\n",
    "        print(\"%d : done %d games, reward %.3f, eps %.2f, speed %.2f f/s\" % (frame_idx, len(total_rewards), m_reward, epsilon, speed))\n",
    "\n",
    "        if best_m_reward is None or best_m_reward < m_reward:\n",
    "            torch.save(net.state_dict(), \"best_%.0f.pth\" % m_reward)\n",
    "            if best_m_reward is not None:\n",
    "                print(\"Best reward updated %.3f -> %.3f\" % (best_m_reward, m_reward))\n",
    "            best_m_reward = m_reward\n",
    "\n",
    "    if len(buffer) < REPLAY_START_SIZE:\n",
    "        continue\n",
    "  \n",
    "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
    "        tgt_net.load_state_dict(net.state_dict())\n",
    "        print(\"Sync\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    loss_t = calc_loss(batch, net, tgt_net, device=device)\n",
    "    loss_t.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}