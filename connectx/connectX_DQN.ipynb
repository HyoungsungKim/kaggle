{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit66f7a8720ea44564891eb6b9b39e6c03",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import time\n",
    "\n",
    "from random import choice\n",
    "from kaggle_environments import evaluate, make, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = collections.namedtuple('Experience', field_names=['observation', 'action', 'reward', 'done', 'new_observation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # * Get a number of batch_size experience in a range of len(self.buffer)\n",
    "        # * It does not replace\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "\n",
    "        observation, actions, rewards, dones, next_observation = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(observation), np.array(actions), np.array(rewards, dtype=np.float32), np.array(dones, dtype=np.uint8), np.array(next_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, shape):\n",
    "        return shape.view(shape.size()[0], -1)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=3, padding = 1, stride=4),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=4, padding = 1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding = 1, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, input_shape):\n",
    "        # * To make 3D shape, we have to put 1 in torch.zeros\n",
    "        # * torch.zeros(1, 1, 52, 52)\n",
    "        # * -> troch.zeros(batch_dimension, *shape)\n",
    "        out = self.conv(torch.zeros(1, *input_shape))\n",
    "        return int(np.prod(out.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x)\n",
    "        fc_out = self.fc(conv_out)\n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "DQN(\n  (conv): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(4, 4), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n    (3): ReLU()\n    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (5): ReLU()\n    (6): Flatten()\n  )\n  (fc): Sequential(\n    (0): Linear(in_features=2304, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n"
    }
   ],
   "source": [
    "image = torch.rand(1, 52, 52)\n",
    "dqn = DQN(image.shape, 10)\n",
    "print(dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /kaggle_environments/envs/connectx.py\n",
    "def is_win(board, column, mark, config, has_played=True):\n",
    "    columns = config.columns\n",
    "    rows = config.rows\n",
    "    inarow = config.inarow - 1\n",
    "    row = (\n",
    "        min([r for r in range(rows) if board[column + (r * columns)] == mark])\n",
    "        if has_played\n",
    "        else max([r for r in range(rows) if board[column + (r * columns)] == 0])\n",
    "    )\n",
    "\n",
    "    def count(offset_row, offset_column):\n",
    "        for i in range(1, inarow + 1):\n",
    "            r = row + offset_row * i\n",
    "            c = column + offset_column * i\n",
    "            if (\n",
    "                r < 0\n",
    "                or r >= rows\n",
    "                or c < 0\n",
    "                or c >= columns\n",
    "                or board[c + (r * columns)] != mark\n",
    "            ):\n",
    "                return i - 1\n",
    "        return inarow\n",
    "\n",
    "    return (\n",
    "        count(1, 0) >= inarow  # vertical.\n",
    "        or (count(0, 1) + count(0, -1)) >= inarow  # horizontal.\n",
    "        or (count(-1, -1) + count(1, 1)) >= inarow  # top left diagonal.\n",
    "        or (count(-1, 1) + count(1, -1)) >= inarow  # top right diagonal.\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env, exp_buffer, mode=\"negamax\"):\n",
    "        configuration = env.configuration\n",
    "        self.env = env\n",
    "        self.columns = configuration['columns']\n",
    "        self.rows = configuration['rows']\n",
    "\n",
    "        self.trainer = env.train([None, mode])\n",
    "        self.exp_buffer = exp_buffer\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self):        \n",
    "        env_observation = self.trainer.reset()\n",
    "        self.mark = env_observation['mark']\n",
    "        self.board = env_observation['board']\n",
    "        np_board = np.array(self.board)\n",
    "        np_board = np_board.reshape(1, 1, self.rows, -1)\n",
    "        assert np_board.shape[3] == self.columns\n",
    "\n",
    "        self.env.reset()\n",
    "        self.observation = np_board\n",
    "        self.total_reward = 0.0\n",
    "\n",
    "    def _select_random_action(self):   \n",
    "        return choice([c for c in range (self.columns) if self.board[c] == 0])  \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def play_step(self, net, epsilon=0.0, device=\"cpu\"):\n",
    "        done_reward = None\n",
    "\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self._select_random_action()\n",
    "        else:            \n",
    "            observation_v = torch.from_numpy(self.observation).float().to(device)\n",
    "            q_vals_v = net(observation_v)\n",
    "            _, action = torch.max(q_vals_v, dim=1)\n",
    "            action = int(action)\n",
    "\n",
    "        new_observation, reward, done, _ = self.trainer.step(action)        \n",
    "        print(action)\n",
    "        \n",
    "        # Do not need to consider lose case. If agent can get high reward when it doen win\n",
    "        if is_win(self.board, action, self.mark, self.env.configuration, has_played=False):\n",
    "            reward = 10 \n",
    "\n",
    "        if done == False:\n",
    "            reward = -1\n",
    "\n",
    "        if reward == None:\n",
    "            reward = -10                                                \n",
    "\n",
    "        self.total_reward += reward\n",
    "        self.board = new_observation['board']\n",
    "\n",
    "        new_observation = np.array(self.board)\n",
    "        new_observation = new_observation.reshape(1, 1, self.rows, -1)\n",
    "        assert new_observation.shape[3] == self.columns\n",
    "        \n",
    "        exp = Experience(self.observation, action, reward, done, new_observation)\n",
    "\n",
    "        self.exp_buffer.append(exp)\n",
    "        self.observation = new_observation\n",
    "\n",
    "        if done:\n",
    "            done_reward = self.total_reward\n",
    "            self._reset()\n",
    "\n",
    "        return done_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n\n"
    }
   ],
   "source": [
    "test_env = make(\"connectx\", debug=True)\n",
    "test_trainer = test_env.train([None, \"negamax\"])\n",
    "observation = test_trainer.reset()\n",
    "test_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = test_env.configuration['columns']\n",
    "rows = test_env.configuration['rows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n\n"
    }
   ],
   "source": [
    "test_buffer = ExperienceBuffer(5)\n",
    "agent = Agent(test_env, test_buffer)\n",
    "epsilon = 1\n",
    "input_shape = [1, rows, columns]\n",
    "n_actions = columns\n",
    "net = DQN(input_shape, n_actions)\n",
    "test_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2\n"
    }
   ],
   "source": [
    "action = agent.play_step(net, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---+---+---+---+---+---+---+\n| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 2 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 2 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 1 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 2 | 0 | 0 | 0 | 0 | 0 | 0 |\n+---+---+---+---+---+---+---+\n| 1 | 2 | 1 | 0 | 0 | 1 | 0 |\n+---+---+---+---+---+---+---+\n\n"
    }
   ],
   "source": [
    "test_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss(batch, net, tgt_net, device='cpu'):\n",
    "    observation, actions, rewards, dones, next_observation = batch\n",
    "\n",
    "    observation_v = torch.from_numpy(observation).float().to(device)\n",
    "    next_observation_v = torch.from_numpy(next_observation).float().to(device)\n",
    "    action_v = torch.from_numpy(actions).to(device)\n",
    "    rewards_v = torch.from_numpy(rewards).to(device)\n",
    "    done_mask = torch.from_numpy(dones, dtype=torch.bool).to(device)\n",
    "\n",
    "    state_action_value = net(observation_v).gather(1, action_v.unsqueeze(-1)).squeeze(-1)\n",
    "    with torch.no_grad():\n",
    "        next_observation_values = tgt_net(next_observation_v).max(1)[0]\n",
    "        next_observation_values[done_mask] = 0.0\n",
    "        next_observation_values = next_observation_values.detach()\n",
    "\n",
    "    expected_state_action_values = next_observation_values * GAMMA + rewards_v\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    return loss(state_action_value, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"connectx\", debug=True)\n",
    "net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)    \n",
    "tgt_net = dqn_model.DQN(env.observation_space.shape, env.action_space.n).to(device)"
   ]
  }
 ]
}